**LiteLLM** is an open-source Python library that acts as a **unified interface (or wrapper)** for calling multiple **Large Language Model (LLM)** providers through a single, standardized API â€” just like OpenAIâ€™s API.

---

## ğŸ§  Think of LiteLLM as:

> A **universal adapter** for different LLMs â€” so you donâ€™t have to learn or manage multiple APIs separately.

---

## ğŸ”§ What It Does:

* Lets you call models from:

  * **OpenAI** (`gpt-3.5-turbo`, `gpt-4`)
  * **Anthropic** (`claude-3-opus`, `claude-3-sonnet`)
  * **Cohere**, **Mistral**, **Ollama**, **Together AI**, **Groq**, and more
* Uses one **OpenAI-like interface** for everything
* Handles:

  * Authentication
  * Rate limiting
  * Model switching
  * Proxying requests

---

## âœ… Example Usage:

```python
from litellm import completion

response = completion(
    model="claude-3-opus",  # Could also be "gpt-3.5-turbo" or "mistral-7b"
    messages=[{"role": "user", "content": "What's the capital of France?"}]
)

print(response['choices'][0]['message']['content'])
```

This looks like OpenAIâ€™s API â€” but under the hood, itâ€™s calling **Anthropic**, **Mistral**, or another provider depending on the `model` value.

---

## ğŸ”Œ Why People Use LiteLLM:

* ğŸš€ **Model Flexibility**: Easily switch models/providers without rewriting logic
* âš¡ **Rate Limit Handling**: Built-in retry and rate limit support
* ğŸ” **Unified Key/Token Management**: Simplifies managing API keys
* ğŸ›¡ **Proxy Mode**: Can run as a proxy server to standardize API calls across apps

---

## ğŸ” Who Is It For?

* Devs building LLM apps who want to **experiment with or switch between models**
* Teams managing multiple LLM vendors
* People deploying **LLMs in production** and need robustness + flexibility

---

## ğŸ“¦ Install:

```bash
pip install litellm
```

---

Would you like to see how to use it with LangChain or in a Flask/FastAPI app?
